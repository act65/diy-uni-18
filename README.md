
Self-taught course in data science and related math.

## Semester 1

#### Math 441: (Online and/or convex) Optimisation

Online optimisation, aka memory limited optimisation?
Would be cool to search for bounds/fastest descent possible.
* What is up with potential functions!?
* Regularisers as projections into the 'right' geometry.
* What are the distinct problems that non-convex optimisation brings?

Readings

* [Potential-Function Proofs for First-Order Methods](https://arxiv.org/abs/1712.04581)
* Time and memory complexity
* Case study on [ADAM]() and its decendants; [NADAM]() and [AMSGRAD]()
* Lagrange dual

Projects (dont seem related to the topic/readings...)

* Implement streaming PCA
* Hierarchical ??? Online frequency. Efficient memory in online setting. Hierarchical sketch.
* Local gradient statistics <!-- Why is the necessary? Pathological surfaces that make point estimates useless -->
* Kronecker-factored approximation (Higher order gradients)

#### Data science 452: Credit assignment

* What conditions make assigning credit hard?
* Why do we want gradients/why do we care?
* What problem does AD solve?

Readings

* [Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation]()
* [Backprop as Functor](https://arxiv.org/abs/1711.10455)
* [Learning long-term dependencies w GD is difficult](http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf)
* Temporal difference

<!-- * [Understanding Synthetic gradients]() -->

Projects

* Implement [doubly recursive AD](http://dankalman.net/preprints/mmgautodiff.pdf)
* Implement efficient graph based reverse AD
* Long-term dependencies: pathological settings and the relationship to entropy
* Alternative credit assignment -- ???

## Semester 2

#### Eng 377: (Graph) signal processing

* Sparse representations: The fourier transformation
* The laplacian
* Graph embeddings for ML

#### Stats 323: (Statistical) learning theory

* Sample complexity
* Assumptions about the data (IID, noise, ...)
* Complexity measures and bounding generalisation

## Semester 3

#### Math 447: Tensor networks

Readings

* [Strassen's Algorithm for Tensor Contraction](https://arxiv.org/abs/1704.03092)
* Case study: Singular value decomposition(s) - HOSVD, HSVD


Projects

* Reshaping tensors, what is this really doing?
* Implement and explore the properties of a complex tensor network


#### Data science 453: Learning discrete models

<!-- What about learning PGMs -->

Reading

* Non differentiable ops: Rebar, concrete, <!-- Gradient estimation through non-differentiable operations and various data structures
 -->
* Alternatives and approximations to gradients
* Combinatorial optimisation
* Learning discretised networks (ES, distillation, quantisation, ...?)

* Extract an automata from a learned network
* Discretise ...
* ?

## Semester 4

#### Alternative ...?

* Hungarian Layer, OptNet, [Submodular](https://papers.nips.cc/paper/6702-differentiable-learning-of-submodular-functions)
* 




#### Natural language processing
<!-- Could spend a whole year on this... Linguistics, evolution of language, programming languages, types, ... ?-->

Readings

* Translating without parallel data
* Question answering
* Summarisation
* Entailment

Projects

* 

## Semester 5

#### ?



#### Reinforcement learning


## Semester 6

#### Meta and recursive learning


#### Representation/approximation theory



## Project

> Automated science and math.

Math and science have become too big for individuals. We find it hard to keep up and to cram the relevant knowledge into our small heads. We need better tools to continue push the boundaries.

## Dates

* Semester 1: March 5th - June 8th
* Semester 2: July 2nd - October 12th
* Semester 3: March 5th - June 8th
* Semester 4: July 2nd - October 12th
* Semester 5: March 5th - June 8th
* Semester 6: July 2nd - October 12th
